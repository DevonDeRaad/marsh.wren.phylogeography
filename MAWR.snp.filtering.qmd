---
title: "MAWR.SNP.filtering"
format: html
toc: true
toc-title: Document Contents
number-sections: true
embed-resources: true
---

### load relevant R packages

```{r}
library(vcfR) #v1.14.0
library(ggplot2) #v3.4.1
library(adegenet) #v2.1.10
library(SNPfiltR) #v1.0.1
library(StAMPP) #v1.6.3
```

We will follow the SNP filtering protocol outlined by the [SNPfiltR](https://devonderaad.github.io/SNPfiltR/) R package in [@deraad2022].

### Read in data {style="color: black"}

We will read in the unfiltered SNPs for all samples output as a vcf file after using BWA [@li2009] to map our raw RADseq data to the *Thryothorus ludovicianus* (Carolina wren) reference genome (available at this NCBI [link](https://www.ncbi.nlm.nih.gov/datasets/genome/GCA_013397245.1/); [@feng2020]).

```{r}
#| output: false
#read in vcf
vcfR <- read.vcfR("~/Desktop/marsh.wren.phylogeography/populations.snps.vcf")
```

```{r}
#how many samples and SNPs did we get from mapping all samples to the CAWR reference genome?
vcfR
```

```{r}
#read in sample info csv
sample.info<-read.csv("~/Desktop/marsh.wren.phylogeography/MAWR.phylogeography.full.sampling.csv")

#reorder sampling file to match order of samples in vcf
sample.info<-sample.info[match(colnames(vcfR@gt)[-1], sample.info$Sample),]
#make sure sampling file matches the order of samples in your vcf
sample.info$Sample == colnames(vcfR@gt)[-1]
```

## Implement quality filters that don't involve missing data

This is because removing low data samples will alter percentage/quantile based missing data cutoffs, so we wait to implement those until after deciding on our final set of samples for downstream analysis

```{r}
#hard filter to minimum depth of 5, and minimum genotype quality of 30
vcfR<-hard_filter(vcfR=vcfR, depth = 3, gq = 30)
```

Use the function `allele_balance()` to filter for allele balance from [Puritz SNP filtering tutorial](http://www.ddocent.com/filtering/) "Allele balance: a number between 0 and 1 representing the ratio of reads showing the reference allele to all reads, considering only reads from individuals called as heterozygous, we expect that the allele balance in our data (for real loci) should be close to 0.5"

```{r}
#execute allele balance filter
vcfR<-filter_allele_balance(vcfR, min.ratio = .10, max.ratio = .90)
```

We now want to implement a max depth filter (super high depth loci are likely multiple loci stuck together into a single paralogous locus, which we want to remove).

```{r}
#visualize and pick appropriate max depth cutoff
max_depth(vcfR)

#filter vcf by the max depth cutoff you chose
vcfR<-max_depth(vcfR, maxdepth = 250)
```

Remove SNPs from the vcfR that have become invariant following the removal of questionable genotypes above, and see how many SNPs we have left after this initial set of filters

```{r}
vcfR<-min_mac(vcfR, min.mac = 1)
vcfR
```

## Setting the missing data by sample threshold

```{r}
#run function to visualize per sample missingness
miss<-missing_by_sample(vcfR)
```

```{r}
#run function to visualize per SNP missingness
by.snp<-missing_by_snp(vcfR)
```

Based on these visualizations, we will want to drop the worst sequenced samples that are dragging down the rest of the dataset. Drop those samples based on an approximate missing data proportion cutoff here (this can always be revised if we end up feeling like this is too lenient or stringent later):

```{r}
#run function to drop samples above the threshold we want from the vcf
vcfR.trim<-missing_by_sample(vcfR=vcfR, cutoff = .9)
#remove invariant sites generated by sample trimming
vcfR.trim<-min_mac(vcfR.trim, min.mac = 1)
```

## Setting the missing data by SNP threshold

Now we will visualize different per SNP missing data thresholds and identify a value that optimizes the trade-off between amount of missing data and the total number of SNPs retained.

```{r}
#see what effect trimming samples had on missing data across the dataset
by.snp<-missing_by_snp(vcfR.trim)
vcf.85<-missing_by_snp(vcfR.trim, cutoff=.85)
miss<-missing_by_sample(vcf.85)
#no samples would have > 50% missing data at an 85% completeness cutoff, so I am happy with that

vcf.90<-missing_by_snp(vcfR.trim, cutoff=.9)
miss<-missing_by_sample(vcf.90)
#no samples would have > 40% missing data at an 90% completeness cutoff
```

Investigate the effect of missing data

```{r}
#I will now investigate and make sure sample clustering isnt being obviously driven by missing data
#make a splitstree for the 90% completeness dataset
#convert to genlight
gen<-vcfR2genlight(vcf.90)
#assign populations (a StaMPP requirement)
gen@pop<-as.factor(gen@ind.names)
gen@ind.names<-gsub("2511-03326_rep2","R2_03326",gen@ind.names)
#generate pairwise divergence matrix
sample.div <- stamppNeisD(gen, pop = FALSE)
#export for splitstree
stamppPhylip(distance.mat=sample.div, file="~/Desktop/marsh.wren.phylogeography/90.splits.txt")

#filter to 100% completeness and make a splitstree
vcf.100<-missing_by_snp(vcfR.trim, cutoff = 1)
#convert to genlight
gen<-vcfR2genlight(vcf.100)
#assign populations (a StaMPP requirement)
gen@pop<-as.factor(gen@ind.names)
gen@ind.names<-gsub("2511-03326_rep2","R2_03326",gen@ind.names)
#generate pairwise divergence matrix
sample.div <- stamppNeisD(gen, pop = FALSE)
#export for splitstree
stamppPhylip(distance.mat=sample.div, file="~/Desktop/marsh.wren.phylogeography/100.splits.txt")
```

![90% complete splitstree (\~18K SNPs)](images/Screenshot%202023-09-20%20at%205.34.24%20PM.png)

![100% complete splitstree (\~1,500 SNPs)](images/Screenshot%202023-09-20%20at%205.35.26%20PM-01.png)

We can see that the percentage of missing data is not causing the intermediate clustering, specifically for the one obvious hybrid sample. Additionally, the samples from North Dakota and Saskatchewan are consistently leaking toward the middle, even when there is no missing data in the input SNP matrix, indicating that these samples have real mismatched alleles, not just excess missing data and insufficient signal to assign them.

## Remove technical replicates

We can also see that for the one sample for which both replicates passed filtering requirements (2511-03326), the two technical replicates are nearly identical in our 90% complete splitstree

![](images/Screenshot%202023-09-20%20at%205.46.33%20PM.png)

In fact, we can see exactly how many genotypes were called the same between the two samples, as an estimate of error rate

```{r}
#calculate number of conflicting genotype calls between technical replicates
table(gsub(":.*","",(vcf.90@gt[,colnames(vcf.90@gt) == "2511-03326_rep1"])) == gsub(":.*","",(vcf.90@gt[,colnames(vcf.90@gt) == "2511-03326_rep2"])))

#isolate the exact genotypes that conflict
z<-gsub(":.*","",(vcf.90@gt[,colnames(vcf.90@gt) == "2511-03326_rep1" | colnames(vcf.90@gt) == "2511-03326_rep2"]))[gsub(":.*","",(vcf.90@gt[,colnames(vcf.90@gt) == "2511-03326_rep1"])) != gsub(":.*","",(vcf.90@gt[,colnames(vcf.90@gt) == "2511-03326_rep2"])),]
#print them with missing values removed
z[complete.cases(z), ]
```

Only 3 genotypes differ between these two replicates across \> 17K called SNPs. This suggests very low (\~0.017%) variability in genotype calls in our final dataset due to non-biological reasons. Looking at the genotypes, the variation is exclusively in genotypes where the caller disagreed about whether to call a homozygous versus het genotype, the types of calls where confidence can vary depending on read depth and random sampling error of the two alleles. This is overall a very encouraging sign for the quality of this dataset.

We will now remove the technical replicate with the most missing data
```{r}
vcf.90<-vcf.90[,colnames(vcf.90@gt) != "2511-03326_rep2"]
#remove any potentially invariant SNPs
vcf.90<-min_mac(vcf.90, min.mac = 1)
```

## Remove overlapping SNPs

It is a known thing (see [this](https://groups.google.com/g/stacks-users/c/Ag8YyEFe7z0)) that Stacks will not merge SNPs called twice if they are sequenced by separate (but physically overlapping) loci. To account for this, we will simply remove a SNP every time its chromosome and position have already been seen in the dataset with the following code:

```{r}
#generate dataframe containing information for chromosome and bp locality of each SNP
df<-as.data.frame(vcf.90@fix[,1:2])
#calc number of duplicated SNPs to remove
nrow(df) - length(unique(paste(df$CHROM,df$POS)))
#remove duplicated SNPs
vcf.90<-vcf.90[!duplicated(paste(df$CHROM,df$POS)),]
```

## Visualize depth and quality across all retained genotypes

```{r}
#plot depth per snp and per sample
dp <- extract.gt(vcf.90, element = "DP", as.numeric=TRUE)
heatmap.bp(dp, rlabels = FALSE)

#plot genotype quality per snp and per sample
gq <- extract.gt(vcf.90, element = "GQ", as.numeric=TRUE)
heatmap.bp(gq, rlabels = FALSE)
```

## linkage filter

Now filter for linkage (thin dataset to maximum 1 SNP per Kb)

```{r}
#perform linkage filtering to get a reduced vcf with only unlinked SNPs
vcfR.thin<-distance_thin(vcf.90, min.distance = 1000)
```

## write out vcf for downstream analyses

```{r}
#get info for all SNPs passing filtering vcf dataset
vcf.90
#write to disk
vcfR::write.vcf(vcf.90, file = "~/Desktop/marsh.wren.phylogeography/filtered.snps.vcf.gz")

#get info for the thinned SNP dataset
vcfR.thin

#write to disk
vcfR::write.vcf(vcfR.thin, file = "~/Desktop/marsh.wren.phylogeography/filtered.unlinked.snps.vcf.gz")
```
